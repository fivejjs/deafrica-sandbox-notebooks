{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting training data from the ODC <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n",
    "\n",
    "* **Products used:** \n",
    "[s2_l2a](https://explorer.digitalearth.africa/s2_l2a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook will extract training data (feature layers) from the `open-data-cube` using geometries within a shapefile (or geojson). To do this, we rely on a custom `deafrica-sandbox-notebooks` function called `collect_training_data`, contained within the [deafrica_classificationtools](../Scripts/deafrica_classificationtools.py) script.  The goal of this notebook is to familarise users with this function so they can extract the appropriate data for their use-case.\n",
    "\n",
    "1. Preview the polygons in our training data by plotting them on a basemap\n",
    "3. Extract training data from the datacube using  a custom defined feature layer function that we can pass to `collect_training_data`\n",
    "4. Export the training data to disk for use in subsequent scripts\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/datacube/storage/masking.py:4: DeprecationWarning: datacube.storage.masking has moved to datacube.utils.masking\n",
      "  category=DeprecationWarning)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/env/lib/python3.6/site-packages/cligj/__init__.py:17: FutureWarning: cligj 1.0.0 will require Python >= 3.7\n",
      "  warn(\"cligj 1.0.0 will require Python >= 3.7\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import subprocess as sp\n",
    "import geopandas as gpd\n",
    "from datacube.utils.geometry import assign_crs\n",
    "\n",
    "sys.path.append('../../Scripts')\n",
    "from deafrica_plotting import map_shapefile\n",
    "from deafrica_classificationtools import collect_training_data \n",
    "from feature_layer_functions import gm_mads_two_seasons\n",
    "\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "configure_s3_access(aws_unsigned=True, cloud_defaults=True)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `path`: The path to the input shapefile from which we will extract training data.\n",
    "* `field`: This is the name of column in your shapefile attribute table that contains the class labels. **The class labels must be integers**\n",
    "* `ncpus`: Set this value to > 1 to parallize the collection of training data. eg. npus=8. To automatically find the number of cpus run the cell two below this one.\n",
    "\n",
    "> **Note**: With supervised classification, its common to have many, many labelled geometries in the training data. `collect_training_data` can parallelize across the geometries in order to speed up the extracting of training data. Setting `ncpus>1` will automatically trigger the parallelization, however, its best to set `ncpus=1` to begin with to assist with debugging before triggering the parallelization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/Eastern_training_data_20201123.geojson' \n",
    "field = 'Class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Automatically find the number of cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 31\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ncpus = int(float(sp.getoutput('env | grep CPU')[-4:]))\n",
    "except:\n",
    "    ncpus = int(float(sp.getoutput('env | grep CPU')[-3:]))\n",
    "\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview input data\n",
    "\n",
    "We can load and preview our input data shapefile using `geopandas`. The shapefile should contain a column with class labels (e.g. 'class'). These labels will be used to train our model. \n",
    "\n",
    "> Remember, the class labels **must** be represented by `integers`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.49666 -3.30737, 32.49693 -3.30716...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.49314 -3.30836, 32.49382 -3.30847...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.49962 -3.31316, 32.50028 -3.31338...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.51721 -3.10441, 32.51716 -3.10465...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.38058 -2.69827, 32.38091 -2.69820...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                           geometry\n",
       "0      1  POLYGON ((32.49666 -3.30737, 32.49693 -3.30716...\n",
       "1      1  POLYGON ((32.49314 -3.30836, 32.49382 -3.30847...\n",
       "2      1  POLYGON ((32.49962 -3.31316, 32.50028 -3.31338...\n",
       "3      1  POLYGON ((32.51721 -3.10441, 32.51716 -3.10465...\n",
       "4      1  POLYGON ((32.38058 -2.69827, 32.38091 -2.69820..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load input data shapefile\n",
    "input_data = gpd.read_file(path)\n",
    "\n",
    "# Plot first five rows\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data in an interactive map\n",
    "# map_shapefile(input_data, attribute=field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can pass this function to `collect_training_data`.  For each of the geometries in our shapefile we will extract temporal statistics, and elevation data.\n",
    "\n",
    "Because we are now interested in calculating a range of temporal statistics, we will redefine our intial parameters to include a time-series of Sentinel-2 data (whereas above we loaded data from a geomedian composite).  Remember, passing in a `custom_func` to `collect_training_data` means many of the other feature layer parameters are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "products =  ['s2_l2a']\n",
    "time = ('2019-01','2019-12')\n",
    "zonal_stats = 'median' \n",
    "return_coords=True\n",
    "\n",
    "# Set up the inputs for the ODC query\n",
    "measurements =  ['red','blue','green','nir','swir_1','swir_2','red_edge_1','red_edge_2','red_edge_3']\n",
    "resolution = (-20,20)\n",
    "output_crs='epsg:6933'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a new datacube query object\n",
    "query = {\n",
    "    'time': time,\n",
    "    'measurements': measurements,\n",
    "    'resolution': resolution,\n",
    "    'output_crs': output_crs,\n",
    "    'group_by' : 'solar_day',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing data using user supplied custom function\n",
      "Taking zonal statistic: median\n",
      "Collecting training data in parallel mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 365/3175 [11:44<2:36:14,  3.34s/it]Error opening source dataset: https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-cogs/37/N/BA/2019/5/S2A_37NBA_20190502_0_L2A/B12.tif\n",
      " 23%|██▎       | 731/3175 [24:51<1:58:55,  2.92s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_36MUT_20190714_0_L2A/SCL.tif\n",
      " 28%|██▊       | 885/3175 [30:35<1:16:16,  2.00s/it]Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37LCL_20190317_0_L2A/B03.tif\n",
      " 45%|████▌     | 1438/3175 [48:53<38:41,  1.34s/it]  Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MBR_20190109_0_L2A/B02.tif\n",
      " 50%|████▉     | 1580/3175 [53:27<34:14,  1.29s/it]  Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MBR_20190109_0_L2A/B02.tif\n",
      " 52%|█████▏    | 1641/3175 [55:43<35:56,  1.41s/it]  Error opening source dataset: https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-cogs/36/N/XF/2019/5/S2B_36NXF_20190507_1_L2A/B02.tif\n",
      " 52%|█████▏    | 1662/3175 [56:29<18:44,  1.35it/s]  Error opening source dataset: s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2A_37MBR_20190109_0_L2A/B02.tif\n",
      "100%|█████████▉| 3168/3175 [1:50:40<00:14,  2.10s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of possible fails after run 1 = 16.19 %\n",
      "Recollecting samples that failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [18:02<00:00,  2.11s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of possible fails after run 2 = 2.71 %\n",
      "Recollecting samples that failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [03:26<00:00,  2.40s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of possible fails after run 3 = 1.57 %\n",
      "Recollecting samples that failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:06<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of possible fails after run 4 = 1.54 %\n",
      "Recollecting samples that failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [02:07<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 50 rows wth NaNs &/or Infs\n",
      "Output shape:  (3118, 37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "column_names, model_input = collect_training_data(\n",
    "                                    gdf=input_data,\n",
    "                                    products=products,\n",
    "                                    dc_query=query,\n",
    "                                    ncpus=25,\n",
    "                                    return_coords=return_coords,\n",
    "                                    field=field,\n",
    "                                    zonal_stats=zonal_stats,\n",
    "                                    custom_func=gm_mads_two_seasons,\n",
    "                                    clean=True,\n",
    "                                    fail_threshold=0.015,\n",
    "                                    max_retries=4\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Class', 'red_S1', 'blue_S1', 'green_S1', 'nir_S1', 'swir_1_S1', 'swir_2_S1', 'red_edge_1_S1', 'red_edge_2_S1', 'red_edge_3_S1', 'edev_S1', 'sdev_S1', 'bcdev_S1', 'NDVI_S1', 'LAI_S1', 'MNDWI_S1', 'rain_S1', 'red_S2', 'blue_S2', 'green_S2', 'nir_S2', 'swir_1_S2', 'swir_2_S2', 'red_edge_1_S2', 'red_edge_2_S2', 'red_edge_3_S2', 'edev_S2', 'sdev_S2', 'bcdev_S2', 'NDVI_S2', 'LAI_S2', 'MNDWI_S2', 'rain_S2', 'slope', 'x_coord', 'y_coord']\n",
      "\n",
      "[[      1.         0.09       0.06 ...       2.95 3135520.   -421710.  ]\n",
      " [      1.         0.09       0.06 ...       3.   3135790.   -422500.  ]\n",
      " [      1.         0.14       0.06 ...      19.11 3179340.   -373030.  ]\n",
      " ...\n",
      " [      1.         0.16       0.09 ...       3.33 3682610.    901010.  ]\n",
      " [      0.         0.37       0.16 ...       4.6  4140770.    864210.  ]\n",
      " [      0.         0.22       0.07 ...       3.19 4553690.    982570.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(column_names)\n",
    "print('')\n",
    "print(np.array_str(model_input, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate coordinates\n",
    "\n",
    "By setting `return_coords=True` in the `collect_training_data` function, our training data now has two extra columns called `x_coord` and `y_coord`.  We need to seperate these from our training dataset as they will not be used to train the machine learning model. Instead, these variables will be used to help conduct Spatial K-fold Cross validation (SKVC) and spatially aware test-train-splits in the notebook `3_Train_fit_evaluate_classifier`.  For more information on why this is important, see this [article](https://www.tandfonline.com/doi/abs/10.1080/13658816.2017.1346255?journalCode=tgis20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_variables = ['x_coord', 'y_coord']\n",
    "model_col_indices = [column_names.index(var_name) for var_name in coord_variables]\n",
    "\n",
    "np.savetxt(\"results/training_data/training_data_coordinates_20201123.txt\", model_input[:, model_col_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export training data\n",
    "\n",
    "Once we've collected all the training data we require, we can write the data to disk. This will allow us to import the data in the next step(s) of the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the name and location of the output file\n",
    "output_file = \"results/training_data/gm_mads_two_seasons_training_data_20201123.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab all columns except the x-y coords\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names[0:-2]]\n",
    "#Export files to disk\n",
    "np.savetxt(output_file, model_input[:, model_col_indices], header=\" \".join(column_names[0:-2]), fmt=\"%4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
